<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浅眠一梦,不曾醉酒</title>
  
  
  <link href="https://sognarere.github.io/atom.xml" rel="self"/>
  
  <link href="https://sognarere.github.io/"/>
  <updated>2022-05-15T09:02:31.888Z</updated>
  <id>https://sognarere.github.io/</id>
  
  <author>
    <name>Deemod</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CUDA学习记录(二)：矩阵乘的初步实现</title>
    <link href="https://sognarere.github.io/2022/05/15/CUDA-2/"/>
    <id>https://sognarere.github.io/2022/05/15/CUDA-2/</id>
    <published>2022-05-15T07:31:10.000Z</published>
    <updated>2022-05-15T09:02:31.888Z</updated>
    
    <content type="html"><![CDATA[<h1 id="矩阵类"><a href="#矩阵类" class="headerlink" title="矩阵类"></a>矩阵类</h1><h2 id="实现思路与方法："><a href="#实现思路与方法：" class="headerlink" title="实现思路与方法："></a>实现思路与方法：</h2><ol><li><p>采用模板编程以实现针对不同类型数据的矩阵乘 <em>template&lt;typename T&gt;</em></p></li><li><p>简化实现，使用一维指针 <em>T* ptr</em> 存储二维矩阵，同时存储尺寸 <em>M、N</em></p></li><li><p>构造时指定尺寸大小，利用<em>memset</em>做置0操作：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Matrix</span>(<span class="keyword">size_t</span> <span class="keyword">const</span>&amp; M=<span class="number">0</span>, <span class="keyword">size_t</span> <span class="keyword">const</span>&amp; N=<span class="number">0</span>):<span class="built_in">M</span>(M),<span class="built_in">N</span>(N) </span><br><span class="line">&#123;</span><br><span class="line">ptr = <span class="keyword">new</span> T[M * N];</span><br><span class="line">    <span class="comment">// 从ptr指向的单元开始，将sizeof(T) * M * N个字节置为0；</span></span><br><span class="line"><span class="built_in">memset</span>(ptr, <span class="number">0</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * M * N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>重载[]操作符，使得可以使用 A[x][y]的访问方式：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">// 返回指向第m行的第一个数的指针</span></span><br><span class="line">   <span class="comment">// 假设ptr指向一个4x4大小的矩阵</span></span><br><span class="line">   <span class="comment">//则重载后ptr[2]=&amp;ptr[8],ptr[2][2]=(&amp;ptr[8])[2]=*(ptr+10)</span></span><br><span class="line">T* <span class="keyword">operator</span>[] (<span class="keyword">size_t</span> m) </span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ptr + m * N;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T* <span class="keyword">const</span> <span class="keyword">operator</span>[] (<span class="keyword">size_t</span> m) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ptr + m * N;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加<em>getPtr</em>成员函数直接获取指针，在运算时将指针数据传入device即可，而无需将整个类传入device</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">T* <span class="title">getPtr</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> ptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">T* <span class="keyword">const</span> <span class="title">getPtr</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>成员函数的const版本供const类型的对象调用</p></li></ol><h2 id="整体代码："><a href="#整体代码：" class="headerlink" title="整体代码："></a>整体代码：</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Matrix</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">Matrix</span>(<span class="keyword">size_t</span> <span class="keyword">const</span>&amp; M=<span class="number">0</span>, <span class="keyword">size_t</span> <span class="keyword">const</span>&amp; N=<span class="number">0</span>):<span class="built_in">M</span>(M),<span class="built_in">N</span>(N) </span><br><span class="line">&#123;</span><br><span class="line">ptr = <span class="keyword">new</span> T[M * N];</span><br><span class="line"><span class="built_in">memset</span>(ptr, <span class="number">0</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * M * N);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T* <span class="keyword">operator</span>[] (<span class="keyword">size_t</span> m) </span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ptr + m * N;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T* <span class="keyword">const</span> <span class="keyword">operator</span>[] (<span class="keyword">size_t</span> m) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> ptr + m * N;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">T* <span class="title">getPtr</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> ptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">T* <span class="keyword">const</span> <span class="title">getPtr</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> ptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> M, N;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">T* ptr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="矩阵乘Kernel的实现"><a href="#矩阵乘Kernel的实现" class="headerlink" title="矩阵乘Kernel的实现"></a>矩阵乘Kernel的实现</h1><p>代码实现了 C[M][N] = A[M][K] * B[N][K] 的矩阵乘运算</p><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><ol><li>思路上没什么好说的，简单地将C矩阵中的各个元素分配给各个线程，算就完事了。</li><li>注意，这个实现方法是最最笨拙，最最朴素的一种实现方法，没有任何优化处理</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">MMK</span><span class="params">(T* <span class="keyword">const</span> A, T* <span class="keyword">const</span> B, T* C, <span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> K)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> idx = blockDim.y * blockIdx.y + threadIdx.y;</span><br><span class="line"><span class="keyword">int</span> idy = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line"><span class="keyword">if</span> (idx &lt; M &amp;&amp; idy &lt; N) &#123;</span><br><span class="line">T tmp = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">tmp += A[idx * K + i]*B[idy*K+i];</span><br><span class="line">&#125;</span><br><span class="line">C[idx * M + idy] = tmp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="矩阵乘Kernel的调用"><a href="#矩阵乘Kernel的调用" class="headerlink" title="矩阵乘Kernel的调用"></a>矩阵乘Kernel的调用</h1><h2 id="实现思路-1"><a href="#实现思路-1" class="headerlink" title="实现思路"></a>实现思路</h2><p>目前我还没有连续或同时调用多个kernel的经历，对于单个的kernel，我个人觉得就分为三步：</p><ol><li>数据准备，Host-&gt;Device</li><li>运行kernel</li><li>写回结果，Device-&gt;Host<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="函数接口"><a href="#函数接口" class="headerlink" title="函数接口"></a>函数接口</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MatrixMultiply</span><span class="params">(Matrix&lt;T&gt;* <span class="keyword">const</span> A, Matrix&lt;T&gt;* <span class="keyword">const</span> B, Matrix&lt;T&gt;* C, <span class="keyword">int</span> BlockSize, <span class="keyword">void</span>*F)</span></span>;</span><br></pre></td></tr></table></figure>说明：</li><li>模板编程适应多种数据类型</li><li>BlockSize指定block的尺寸</li><li>void*F 指向kernel函数的指针，我会实现多个接口一样的矩阵乘kernel，到时候直接调用指针即可，而无需更改MatrixMultiply内的代码。<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">   T *aptr, *bptr, *cptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;aptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * A-&gt;M * A-&gt;N);</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;bptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * B-&gt;M * B-&gt;N);</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;cptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * C-&gt;M * C-&gt;N);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemcpy</span>(aptr, A-&gt;<span class="built_in">getPtr</span>(), <span class="built_in"><span class="keyword">sizeof</span></span>(T) * A-&gt;M * A-&gt;N, cudaMemcpyHostToDevice);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(bptr, B-&gt;<span class="built_in">getPtr</span>(), <span class="built_in"><span class="keyword">sizeof</span></span>(T) * B-&gt;M * B-&gt;N, cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>说明：</li><li>aptr,bptr,cptr这三个指针指向显存，所在内存的单元存储GPU分配的显存首地址，这三个指针位于Host端</li><li><em>cudaMalloc(void** devPtr, size_t size)<em>，在GPU端分配size bytes大小的空间并返回</em>devPtr</em>, <a href="https://blog.csdn.net/qq_37206769/article/details/88311408?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0-88311408-blog-92001270.pc_relevant_default&spm=1001.2101.3001.4242.1&utm_relevant_index=3">使用二重指针的原因</a></li><li>*cudaMemcpy(void* dst , const void* src , size_t size , cudaMemcpyDeviceToHost)*，将src上的 size bytes大小的数据传送至dst上，cudaMemcpyDeviceToHost用于说明src，dst指针位置，表明传送方向。<h3 id="kernel调用"><a href="#kernel调用" class="headerlink" title="kernel调用"></a>kernel调用</h3>将void* F 转回我们要调用的函数指针直接调用即可<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> func = <span class="keyword">static_cast</span>&lt;<span class="built_in"><span class="keyword">void</span></span>(*)(T* <span class="keyword">const</span>, T* <span class="keyword">const</span>, T*, <span class="keyword">int</span>, <span class="keyword">int</span>, <span class="keyword">int</span>)&gt;(F);</span><br><span class="line">func&lt;&lt;&lt;blockNumber,threadPerBlock&gt;&gt;&gt;(aptr, bptr, cptr, C-&gt;M, C-&gt;N, A-&gt;N);</span><br></pre></td></tr></table></figure><h3 id="写回结果"><a href="#写回结果" class="headerlink" title="写回结果"></a>写回结果</h3>将Device端的运算结果写回Host端<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemcpy</span>(C-&gt;<span class="built_in">getPtr</span>(), cptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * C-&gt;M * C-&gt;N, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _MY_KERNEL</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _MY_KERNEL</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;cuda_runtime.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;device_launch_parameters.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;Matrix.cuh&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//C[M][N] = A[M][K] * B[N][K]</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">MMK</span><span class="params">(T* <span class="keyword">const</span> A, T* <span class="keyword">const</span> B, T* C, <span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> K)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> idx = blockDim.y * blockIdx.y + threadIdx.y;</span><br><span class="line"><span class="keyword">int</span> idy = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line"><span class="keyword">if</span> (idx &lt; M &amp;&amp; idy &lt; N) &#123;</span><br><span class="line">T tmp = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; K; i++) &#123;</span><br><span class="line">tmp += A[idx * K + i]*B[idy*K+i];</span><br><span class="line">&#125;</span><br><span class="line">C[idx * M + idy] = tmp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MatrixMultiply</span><span class="params">(Matrix&lt;T&gt;* <span class="keyword">const</span> A, Matrix&lt;T&gt;* <span class="keyword">const</span> B, Matrix&lt;T&gt;* C, <span class="keyword">int</span> BlockSize, <span class="keyword">void</span>*F)</span> </span>&#123;</span><br><span class="line"><span class="keyword">auto</span> func = <span class="keyword">static_cast</span>&lt;<span class="built_in"><span class="keyword">void</span></span>(*)(T* <span class="keyword">const</span>, T* <span class="keyword">const</span>, T*, <span class="keyword">int</span>, <span class="keyword">int</span>, <span class="keyword">int</span>)&gt;(F);</span><br><span class="line">cudaEvent_t start, stop;</span><br><span class="line"><span class="keyword">float</span> elapsedTime = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"></span><br><span class="line"><span class="function">dim3 <span class="title">threadPerBlock</span><span class="params">(BlockSize, BlockSize)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">blockNumber</span><span class="params">((C-&gt;M + threadPerBlock.y - <span class="number">1</span>) / threadPerBlock.y,</span></span></span><br><span class="line"><span class="params"><span class="function">(C-&gt;N + threadPerBlock.x - <span class="number">1</span>) / threadPerBlock.x)</span></span>;</span><br><span class="line"></span><br><span class="line">T *aptr, *bptr, *cptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;aptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * A-&gt;M * A-&gt;N);</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;bptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * B-&gt;M * B-&gt;N);</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">void</span>**)&amp;cptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * C-&gt;M * C-&gt;N);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemcpy</span>(aptr, A-&gt;<span class="built_in">getPtr</span>(), <span class="built_in"><span class="keyword">sizeof</span></span>(T) * A-&gt;M * A-&gt;N, cudaMemcpyHostToDevice);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(bptr, B-&gt;<span class="built_in">getPtr</span>(), <span class="built_in"><span class="keyword">sizeof</span></span>(T) * B-&gt;M * B-&gt;N, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">func&lt;&lt;&lt;blockNumber,threadPerBlock&gt;&gt;&gt;(aptr, bptr, cptr, C-&gt;M, C-&gt;N, A-&gt;N);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaThreadSynchronize</span>();</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemcpy</span>(C-&gt;<span class="built_in">getPtr</span>(), cptr, <span class="built_in"><span class="keyword">sizeof</span></span>(T) * C-&gt;M * C-&gt;N, cudaMemcpyDeviceToHost);</span><br><span class="line">std::cout &lt;&lt; elapsedTime &lt;&lt;<span class="string">&quot; ms&quot;</span>&lt;&lt; std::endl;</span><br><span class="line"><span class="built_in">cudaFree</span>(aptr);</span><br><span class="line"><span class="built_in">cudaFree</span>(bptr);</span><br><span class="line"><span class="built_in">cudaFree</span>(cptr);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">// !1</span></span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;矩阵类&quot;&gt;&lt;a href=&quot;#矩阵类&quot; class=&quot;headerlink&quot; title=&quot;矩阵类&quot;&gt;&lt;/a&gt;矩阵类&lt;/h1&gt;&lt;h2 id=&quot;实现思路与方法：&quot;&gt;&lt;a href=&quot;#实现思路与方法：&quot; class=&quot;headerlink&quot; title=&quot;实现思路与</summary>
      
    
    
    
    <category term="CUDA学习笔记" scheme="https://sognarere.github.io/categories/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>CUDA学习记录(一)：基本概念与语法</title>
    <link href="https://sognarere.github.io/2022/05/01/CUDA-1/"/>
    <id>https://sognarere.github.io/2022/05/01/CUDA-1/</id>
    <published>2022-05-01T06:23:18.000Z</published>
    <updated>2022-05-08T08:56:43.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是CUDA？"><a href="#什么是CUDA？" class="headerlink" title="什么是CUDA？"></a>什么是CUDA？</h1><p>CUDA(Compute Unified Device Architecture)，直译是统一计算设备架构，是NVIDIA发明的一种并行计算平台和编程模型，可利用NVDIA GPU的并行计算能力大幅提升计算性能。</p><h1 id="GPU-与-CPU的区别"><a href="#GPU-与-CPU的区别" class="headerlink" title="GPU 与 CPU的区别"></a>GPU 与 CPU的区别</h1><p><img src="https://s2.loli.net/2022/05/08/Sy7dCpK1NQAImLF.png" alt="GPUCPU.png"><br>两者的主要区别为：</p><ol><li>GPU 并行度更高，有更多的运算单元，擅长处理高度并行的运算，如图像渲染、挖矿。</li><li>CPU 通用性更好，有更多的控制与缓存单元，更擅长逻辑控制，如个人PC。</li></ol><h1 id="CUDA-编程模型"><a href="#CUDA-编程模型" class="headerlink" title="CUDA 编程模型"></a>CUDA 编程模型</h1><h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><p>由CUDA扩展的C++(CUDA C++)所定义的函数即为Kernel，当被调用时，它将并行地在N个线程上执行N次，而普通的C++函数则只会执行一次。</p><p>通常用__global__来定义Kernel函数，用&lt;&lt;&lt;…&gt;&gt;&gt;来的定义执行Kernel的CUDA线程数，每个线程有唯一的thread ID，可通过Kernel的内置变量访问。</p><p>一个简单的例子如下所示，该实例完成了N维向量的加法，每个线程都将执行VecAdd()，函数中通过<em>threadIdx.x</em>获取当前线程的thread ID，以完成对应位置元素相加。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">VecAdd</span><span class="params">(<span class="keyword">float</span>* A, <span class="keyword">float</span>* B, <span class="keyword">float</span>* C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = threadIdx.x;</span><br><span class="line">C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Kernel invocation with N threads</span></span><br><span class="line">VecAdd&lt;&lt;&lt;<span class="number">1</span>, N&gt;&gt;&gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="线程层次结构"><a href="#线程层次结构" class="headerlink" title="线程层次结构"></a>线程层次结构</h2><p>为了表达方便，<em>threadIdx</em>被定义为三元素的向量(<em>thread index</em>)，通过<em>thread index</em>就可以获取当前<em>thread ID</em>，这些<em>thread</em>就构成了<em>Block</em>。</p><p><em>thread index</em>与<em>thread ID</em>是一一对应的关系，假设一个<em>Block</em>大小为($D_x$,$D_y$，$D_z$),则<em>thread index</em>(x,y,z)对应的<em>thread ID</em> = $x+y\times D_x+z\times D_x D_y$ (跟我们平常定义数组各维度的顺序刚好相反)</p><p>由于同一个<em>block</em>上的thread期望在同一个Core上运行，它们将分享Core上有限的memory资源，因此一个<em>block</em>上的线程数是有限的，该上限目前为1024。</p><p>以下示例代码完成了矩阵加法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">MatAdd</span><span class="params">(<span class="keyword">float</span> A[N][N], <span class="keyword">float</span> B[N][N],</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">float</span> C[N][N])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = threadIdx.x;</span><br><span class="line"><span class="keyword">int</span> j = threadIdx.y;</span><br><span class="line">C[i][j] = A[i][j] + B[i][j];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Kernel invocation with one block of N * N * 1 threads</span></span><br><span class="line"><span class="keyword">int</span> numBlocks = <span class="number">1</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(N, N)</span></span>;</span><br><span class="line">MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一个Kernel可被多个<em>block</em>执行，与<em>tread</em>层级相类似，多个<em>block</em>也可以组成一维、二维、三维的结构，即<em>grid</em>，三者关系如下图所示：<br><img src="https://s2.loli.net/2022/05/08/O2YldMJ4zNfrFbj.png" alt="ThreadHierarchy.png"></p><p>以下示例代码展示了由多个block完成的矩阵加法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">MatAdd</span><span class="params">(<span class="keyword">float</span> A[N][N], <span class="keyword">float</span> B[N][N],</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">float</span> C[N][N])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"><span class="keyword">int</span> j = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"><span class="keyword">if</span> (i &lt; N &amp;&amp; j &lt; N)</span><br><span class="line">C[i][j] = A[i][j] + B[i][j];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Kernel invocation</span></span><br><span class="line"><span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">numBlocks</span><span class="params">(N / threadsPerBlock.x, N / threadsPerBlock.y)</span></span>;</span><br><span class="line">MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>block可以完全独立地执行，即block之间没有依赖，可以以任意顺序被任何一个Core调度，如下图所示：<br><img src="https://s2.loli.net/2022/05/08/xhiMaoBb5Fzkmq2.png" alt="separateExec.png"></p><p>同一个<em>block</em>内的thread可通过shared memory来共享数据，也可以通过同步操作来协调内存访问。在kernel中调用内建函数__syncthreads()来声明同步点，该函数相当于一个<em>barrier</em>，所有可以执行到此处的线程都将被阻断在这里，直到它们都执行到这里后才继续执行(Any thread reaching the barrier waits until all of the other threads in that block also reach it. It is designed for avoiding race conditions when loading shared memory, and the compiler will not move memory reads/writes around a __syncthreads())</p><p>为了更有效的线程间合作，shared memory具有非常低的延迟（跟L1 cache非常类似），所以__syncthreads()是非常轻量的。</p><h2 id="存储层次结构"><a href="#存储层次结构" class="headerlink" title="存储层次结构"></a>存储层次结构</h2><p><img src="https://s2.loli.net/2022/05/08/hEzdmaIRYrlUj61.png" alt="MemHierarchy.png"><br>在执行时<em>thread</em>可访问多种存储空间</p><ol><li>local memory:<em>thread</em>具有的私有存储空间。</li><li>shared memory:同一<em>block</em>中的<em>threads</em>可见</li><li>global memory:对所有<em>thread</em>可见</li><li>constant memory:对所有<em>thread</em>可见</li><li>texture memory：对所有<em>thread</em>可见，为某些特定数据格式提供不同的寻址模式以及数据过滤。<h2 id="异构编程"><a href="#异构编程" class="headerlink" title="异构编程"></a>异构编程</h2><img src="https://s2.loli.net/2022/05/08/Qt7PIor2xuLvizC.png" alt="exec.png"></li></ol><p>CUDA默认异构执行，GPU作为Host端的协处理器，即相关kernel在GPU端执行，剩下的代码在CPU端执行。</p><p>执行程序时，host与device都在DRAM中维护自己独立的内存空间(host memory, device memroy)，通过调用CUDA runtime来管理对kernel可见的global/constant/texture memory，包括device端内存的分配与释放以及host与device端的数据传输。</p><p>Unified Memory提供了managed memory来连接host与device的存储空间，managed memory是一段地址连贯的公共存储空间，可被系统中的CPU与GPU访问，这一功能使得编程时无需再显式地在host与device之间传输数据，简化了应用。(实际上还是分开的，只是编程角度看好像统一了)</p><h2 id="异步SIMT编程模型"><a href="#异步SIMT编程模型" class="headerlink" title="异步SIMT编程模型"></a>异步SIMT编程模型</h2><p>CUDA编程模型中最底层的执行单元为<em>thread</em>，操作可分为计算操作与访存操作(个人理解)，可通过异步编程模型来加速程序执行过程中的访存操作。</p><p>异步编程模型为 CUDA 线程之间的同步定义了异步屏障的行为，该模型还解释并定义了如何使用cuda::memcpy_async 在 GPU 中计算时从全局内存中异步读写数据。</p><h3 id="异步操作-这部分没看懂多少！！！！！以后再填坑"><a href="#异步操作-这部分没看懂多少！！！！！以后再填坑" class="headerlink" title="异步操作(这部分没看懂多少！！！！！以后再填坑)"></a>异步操作(这部分没看懂多少！！！！！以后再填坑)</h3><p>异步操作被定义为由一个CUDA线程启动,但看起来像由另一个线程异步执行的操作(An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread.)，就像是一个线程里有一个子线程，这个线程可以在一定范围内不按代码顺序执行任务，比如第n行代码正在执行大量数据拷贝，耗时很长，此时就可以先执行下一行代码，而无需一直等待。</p><p><strong>注意仅仅是看起来像是有另一个线程，实际上并没有创建新的线程！</strong></p><p>在规范的程序中，一个或多个CUDA线程与异步操作同步。启动异步操作的 CUDA线程不需要在同步线程中。</p><p>异步线程始终与启动异步操作的CUDA线程相关联，异步操作使用同步对象来同步操作的完成。 这样的同步对象可以由用户显式管理（例如，cuda::memcpy_async）或在库中隐式管理（例如，cooperative_groups::memcpy_async）</p><p>同步对象可以是 cuda::barrier 或 cuda::pipeline。这些同步对象可以在不同的thread scope内使用。scope定义了可以使用同步对象与异步操作同步的线程集合。下表定义了 CUDA C++ 中可用的线程范围以及可以与之同步的线程。</p><table><thead><tr><th>Thread scope</th><th>描述</th></tr></thead><tbody><tr><td>cuda::thread_scope::thread_scope_thread</td><td>只同步初始化异步操作的线程</td></tr><tr><td>cuda::thread_scope::thread_scope_block</td><td>同步所有或者同一个block中创建的CUDA线程</td></tr><tr><td>cuda::thread_scope::thread_scope_device</td><td>同步所有或者同一个block中创建的CUDA线程</td></tr><tr><td>cuda::thread_scope::thread_scope_system</td><td>同步系统中创建的所有CPU线程或GPU CUDA线程</td></tr></tbody></table><h2 id="Compute-Capability"><a href="#Compute-Capability" class="headerlink" title="Compute Capability"></a>Compute Capability</h2><p>Compute Capability (SM Version)用来描述硬件的计算能力与支持的功能和指令，其格式为X.Y，其中主版本号X代表核心架构，副版本号Y代表基于X架构的增量更新。</p><p>具有相同核心架构的GPU X相同：</p><table><thead><tr><th align="center">Architecture</th><th align="center">Tesla</th><th align="center">Fermi</th><th align="center">Kepler</th><th align="center">Maxwell</th><th align="center">Pascal</th><th align="center">Volta</th><th align="center">Ampere</th></tr></thead><tbody><tr><td align="center">X</td><td align="center">1</td><td align="center">2</td><td align="center">3</td><td align="center">5</td><td align="center">6</td><td align="center">7</td><td align="center">8</td></tr></tbody></table><p>例如，Turing的SM version为7.5，其核心基于Volta。</p><p>查询Compute Capability的网站：<a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a><br>Compute Capability对应支持的功能：<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">CUDA文档</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是CUDA？&quot;&gt;&lt;a href=&quot;#什么是CUDA？&quot; class=&quot;headerlink&quot; title=&quot;什么是CUDA？&quot;&gt;&lt;/a&gt;什么是CUDA？&lt;/h1&gt;&lt;p&gt;CUDA(Compute Unified Device Architecture)，直译是统</summary>
      
    
    
    
    <category term="CUDA学习笔记" scheme="https://sognarere.github.io/categories/CUDA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorRT学习记录(一)：简介与工作流程</title>
    <link href="https://sognarere.github.io/2022/04/10/TensorRT%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B/"/>
    <id>https://sognarere.github.io/2022/04/10/TensorRT%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B/</id>
    <published>2022-04-10T04:13:47.757Z</published>
    <updated>2022-04-10T07:35:33.074Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下学习TensorRT的过程，基本上是提炼翻译了一下手册。</p><h1 id="一、TensorRT-简介"><a href="#一、TensorRT-简介" class="headerlink" title="一、TensorRT 简介"></a>一、TensorRT 简介</h1><p>TensorRT是NVIDIA推出的一款高性能<strong>推理</strong>引擎，可与TensorFlow、Caffe等前端神经网络开发框架组合，加速网络在NVIDIA GPU上的推理速度。</p><h1 id="二、TensorRT-基本工作流程"><a href="#二、TensorRT-基本工作流程" class="headerlink" title="二、TensorRT 基本工作流程"></a>二、TensorRT 基本工作流程</h1><p>TensorRT的运行分为两个阶段：Build、Runtime。</p><h2 id="1、Build"><a href="#1、Build" class="headerlink" title="1、Build"></a>1、Build</h2><p>TensorRT针对目标GPU对网络模型做相应的优化处理，该阶段主要接口为 <em>Builder</em>，由 <em>Builder</em> 执行优化并生成 <em>Engine</em>。</p><p>生成<em>Engine</em>的步骤如下：</p><h3 id="1-创建网络定义"><a href="#1-创建网络定义" class="headerlink" title="(1)创建网络定义"></a>(1)创建网络定义</h3><p>由<em>NetworkDefinition</em> 负责，最常用的方式是在传入 TensorRT 前，先将前端网络模型转换为 ONNX 格式，然后使用 TensorRT 的 ONNX parser 将其传入。</p><h3 id="2-配置Builder"><a href="#2-配置Builder" class="headerlink" title="(2)配置Builder"></a>(2)配置<em>Builder</em></h3><p>由 <em>BuilderConfig</em> 负责，定义 <em>Builder</em> 如何优化网络模型。</p><h3 id="3-调用Builder生成Engine"><a href="#3-调用Builder生成Engine" class="headerlink" title="(3)调用Builder生成Engine"></a>(3)调用Builder生成<em>Engine</em></h3><p>完成网络定义与 <em>Builder</em> 配置后,即可调用 <em>Builder</em> 生成 <em>Engine</em>，<em>Builder</em>以序列化的方式创建 <em>Engine</em>（序列化后为Plan）， Plan可反序列化或保存至磁盘供后续使用。</p><p>P.S 序列化是将对象转换为可保持或可传输的格式的过程，与之相对的是反序列化。其将流还原为对象。</p><h2 id="2、Runtime"><a href="#2、Runtime" class="headerlink" title="2、Runtime"></a>2、Runtime</h2><p>该阶段执行优化后的神经网络模型，主要接口为<em>Runtime</em>，步骤如下：</p><h3 id="1-反序列化-Plan，创建-Engine"><a href="#1-反序列化-Plan，创建-Engine" class="headerlink" title="(1)反序列化 Plan，创建 Engine"></a>(1)反序列化 Plan，创建 <em>Engine</em></h3><p><em>Engine</em> 即为优化后的网络模型。</p><h3 id="2-创建ExecutionContext"><a href="#2-创建ExecutionContext" class="headerlink" title="(2)创建ExecutionContext"></a>(2)创建ExecutionContext</h3><p><em>ExecutionContext</em> 由 <em>Engine</em> 创建，是调用推理的主要接口。</p><h3 id="3-设置输入-输出-buffer"><a href="#3-设置输入-输出-buffer" class="headerlink" title="(3)设置输入/输出 buffer"></a>(3)设置输入/输出 buffer</h3><p>调用推理前必须设置输入输出buffer。</p><h3 id="4-执行推理"><a href="#4-执行推理" class="headerlink" title="(4)执行推理"></a>(4)执行推理</h3><p>以上准备就绪后，执行推理。</p><h1 id="三、SampleOnnxMNIST代码走读"><a href="#三、SampleOnnxMNIST代码走读" class="headerlink" title="三、SampleOnnxMNIST代码走读"></a>三、SampleOnnxMNIST代码走读</h1><p>这部分代码为官方实例代码，实现了加速 ONNX 格式的MNIST网络模型。</p><p>SampleOnnxMNIST类实现了网络模型的读取、优化及运行，接口如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleOnnxMNIST</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">SampleOnnxMNIST</span>(<span class="keyword">const</span> samplesCommon::OnnxSampleParams&amp; params)</span><br><span class="line">        : <span class="built_in">mParams</span>(params)</span><br><span class="line">        , <span class="built_in">mEngine</span>(<span class="literal">nullptr</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="comment">//! \brief Function builds the network engine</span></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">build</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="comment">//! \brief Runs the TensorRT inference engine for this sample</span></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">infer</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    samplesCommon::OnnxSampleParams mParams; <span class="comment">//!&lt; The parameters for the sample.</span></span><br><span class="line"></span><br><span class="line">    nvinfer1::Dims mInputDims;  <span class="comment">//!&lt; The dimensions of the input to the network.</span></span><br><span class="line">    nvinfer1::Dims mOutputDims; <span class="comment">//!&lt; The dimensions of the output to the network.</span></span><br><span class="line">    <span class="keyword">int</span> mNumber&#123;<span class="number">0</span>&#125;;             <span class="comment">//!&lt; The number to classify</span></span><br><span class="line"></span><br><span class="line">    std::shared_ptr&lt;nvinfer1::ICudaEngine&gt; mEngine; <span class="comment">//!&lt; The TensorRT engine used to run the network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="comment">//! \brief Parses an ONNX model for MNIST and creates a TensorRT network</span></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">constructNetwork</span><span class="params">(SampleUniquePtr&lt;nvinfer1::IBuilder&gt;&amp; builder,</span></span></span><br><span class="line"><span class="params"><span class="function">        SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;&amp; network, SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;&amp; config,</span></span></span><br><span class="line"><span class="params"><span class="function">        SampleUniquePtr&lt;nvonnxparser::IParser&gt;&amp; parser)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="comment">//! \brief Reads the input  and stores the result in a managed buffer</span></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">processInput</span><span class="params">(<span class="keyword">const</span> samplesCommon::BufferManager&amp; buffers)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="comment">//! \brief Classifies digits and verify result</span></span><br><span class="line">    <span class="comment">//!</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">verifyOutput</span><span class="params">(<span class="keyword">const</span> samplesCommon::BufferManager&amp; buffers)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="1、OnnxSampleParams"><a href="#1、OnnxSampleParams" class="headerlink" title="1、OnnxSampleParams"></a>1、OnnxSampleParams</h2><p>OnnxSampleParams为一个结构体继承了SampleParams，保存了输入、输出、<em>builder</em>的优化配置等信息，结构如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SampleParams</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int32_t</span> batchSize&#123;<span class="number">1</span>&#125;;              <span class="comment">//!&lt; Number of inputs in a batch</span></span><br><span class="line">    <span class="keyword">int32_t</span> dlaCore&#123;<span class="number">-1</span>&#125;;               <span class="comment">//!&lt; Specify the DLA core to run network on.</span></span><br><span class="line">    <span class="keyword">bool</span> int8&#123;<span class="literal">false</span>&#125;;                  <span class="comment">//!&lt; Allow runnning the network in Int8 mode.</span></span><br><span class="line">    <span class="keyword">bool</span> fp16&#123;<span class="literal">false</span>&#125;;                  <span class="comment">//!&lt; Allow running the network in FP16 mode.</span></span><br><span class="line">    std::vector&lt;std::string&gt; dataDirs; <span class="comment">//!&lt; Directory paths where sample data files are stored</span></span><br><span class="line">    std::vector&lt;std::string&gt; inputTensorNames;</span><br><span class="line">    std::vector&lt;std::string&gt; outputTensorNames;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">OnnxSampleParams</span> :</span> <span class="keyword">public</span> SampleParams</span><br><span class="line">&#123;</span><br><span class="line">    std::string onnxFileName; <span class="comment">//!&lt; Filename of ONNX file of a network</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2、build"><a href="#2、build" class="headerlink" title="2、build()"></a>2、build()</h2><h3 id="1-创建builder"><a href="#1-创建builder" class="headerlink" title="(1)创建builder"></a>(1)创建<em>builder</em></h3><p>创建 <em>builder</em> 的代码如下所示</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> builder = SampleUniquePtr&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>SampleUniquePtr为TensorRT包装的unique类型智能指针，其定义如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">using</span> SampleUniquePtr = std::unique_ptr&lt;T, InferDeleter&gt;;</span><br></pre></td></tr></table></figure><p>createInferBuilder函数，返回IBuilder类型指针：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> IBuilder* <span class="title">createInferBuilder</span><span class="params">(ILogger&amp; logger)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;IBuilder*&gt;(<span class="built_in">createInferBuilder_INTERNAL</span>(&amp;logger, NV_TENSORRT_VERSION));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数的输入参数ILogger为日志接口，用于记录运行时的相关信息(?)，这部分以后再看看吧</p><h3 id="2-创建网络定义"><a href="#2-创建网络定义" class="headerlink" title="(2)创建网络定义"></a>(2)创建网络定义</h3><p>通过<em>Builder</em>的createNetworkV2函数返回网络定义对象，该函数的输入参数定义了网络对象的一些属性，explicitBatch表示该网络为显式固定batch，代码如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">auto</span> explicitBatch = <span class="number">1U</span> &lt;&lt; <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);</span><br><span class="line"><span class="keyword">auto</span> network = SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(explicitBatch));</span><br><span class="line"><span class="keyword">if</span> (!network)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-创建Builder配置"><a href="#3-创建Builder配置" class="headerlink" title="(3)创建Builder配置"></a>(3)创建<em>Builder</em>配置</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> config = SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line"><span class="keyword">if</span> (!config)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-创建-ONNX-Parser"><a href="#4-创建-ONNX-Parser" class="headerlink" title="(4)创建 ONNX Parser"></a>(4)创建 ONNX Parser</h3><p>由于模型为 ONNX 格式，因此需要用 TensorRT 的ONNX Parser来解析并将其读入。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> parser</span><br><span class="line">    = SampleUniquePtr&lt;nvonnxparser::IParser&gt;(nvonnxparser::<span class="built_in">createParser</span>(*network, sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line"><span class="keyword">if</span> (!parser)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中函数createParser将返回一个parser对象，解析的结果将写入之前定义的network中。</p><h3 id="5-构建网络"><a href="#5-构建网络" class="headerlink" title="(5)构建网络"></a>(5)构建网络</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> constructed = <span class="built_in">constructNetwork</span>(builder, network, config, parser);</span><br><span class="line"><span class="keyword">if</span> (!constructed)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调用constructNetwork函数完成网络的构建，在函数内调用之前创建的ONNX解析器对解析ONNX文件，并完成对<em>builder</em>的配置，代码如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SampleOnnxMNIST::constructNetwork</span><span class="params">(SampleUniquePtr&lt;nvinfer1::IBuilder&gt;&amp; builder,</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;&amp; network, SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;&amp; config,</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvonnxparser::IParser&gt;&amp; parser)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 解析 ONNX 文件，获得网络模型</span></span><br><span class="line">    <span class="keyword">auto</span> parsed = parser-&gt;<span class="built_in">parseFromFile</span>(<span class="built_in">locateFile</span>(mParams.onnxFileName, mParams.dataDirs).<span class="built_in">c_str</span>(),</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(sample::gLogger.<span class="built_in">getReportableSeverity</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!parsed)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置TensorRT 工作空间大小，越大可采用的优化算法越多</span></span><br><span class="line">    <span class="comment">// 示例中设为16MB</span></span><br><span class="line">    config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">16</span>_MiB);</span><br><span class="line">    <span class="comment">// 配置参数是否为FP16，INT8类型</span></span><br><span class="line">    <span class="keyword">if</span> (mParams.fp16)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mParams.int8)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">        samplesCommon::<span class="built_in">setAllDynamicRanges</span>(network.<span class="built_in">get</span>(), <span class="number">127.0f</span>, <span class="number">127.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 目标硬件是否DLA</span></span><br><span class="line">    samplesCommon::<span class="built_in">enableDLA</span>(builder.<span class="built_in">get</span>(), config.<span class="built_in">get</span>(), mParams.dlaCore);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>P.S. DLA(Deep Learning Accelerator)是NVIDIA开源的深度学习加速器。</p><h3 id="6-配置-CUDA-流"><a href="#6-配置-CUDA-流" class="headerlink" title="(6)配置 CUDA 流"></a>(6)配置 CUDA 流</h3><p>为config配置用于执行该网络的CUDA流：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line"><span class="keyword">if</span> (!profileStream)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br></pre></td></tr></table></figure><h3 id="7-生成-Engine"><a href="#7-生成-Engine" class="headerlink" title="(7)生成 Engine"></a>(7)生成 <em>Engine</em></h3><ol><li>根据网络定义与builder配置编译优化网络，并将其序列化得到plan</li><li>创建runtime准备反序列化</li><li>反序列化plan获得Engine<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据之前的网络定义与builder配置编译优化并序列化网络</span></span><br><span class="line">SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line"><span class="keyword">if</span> (!plan)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建runtime准备反序列化</span></span><br><span class="line">SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line"><span class="keyword">if</span> (!runtime)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 反序列化plan获得Engine</span></span><br><span class="line">mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">    runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(plan-&gt;<span class="built_in">data</span>(), plan-&gt;<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line"><span class="keyword">if</span> (!mEngine)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SampleOnnxMNIST::build</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> builder = SampleUniquePtr&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span> explicitBatch = <span class="number">1U</span> &lt;&lt; <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);</span><br><span class="line">    <span class="keyword">auto</span> network = SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(explicitBatch));</span><br><span class="line">    <span class="keyword">if</span> (!network)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> config = SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line">    <span class="keyword">if</span> (!config)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> parser</span><br><span class="line">        = SampleUniquePtr&lt;nvonnxparser::IParser&gt;(nvonnxparser::<span class="built_in">createParser</span>(*network, sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!parser)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> constructed = <span class="built_in">constructNetwork</span>(builder, network, config, parser);</span><br><span class="line">    <span class="keyword">if</span> (!constructed)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line">    <span class="keyword">if</span> (!profileStream)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!plan)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!runtime)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">        runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(plan-&gt;<span class="built_in">data</span>(), plan-&gt;<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line">    <span class="keyword">if</span> (!mEngine)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbInputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mInputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(mInputDims.nbDims == <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbOutputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mOutputDims = network-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(mOutputDims.nbDims == <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、infer"><a href="#3、infer" class="headerlink" title="3、infer()"></a>3、infer()</h2><p>这部分对应于之前介绍的Runtime阶段，主要任务是：</p><h3 id="1-创建-ExecutionContext"><a href="#1-创建-ExecutionContext" class="headerlink" title="(1)创建 ExecutionContext"></a>(1)创建 ExecutionContext</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> context = SampleUniquePtr&lt;nvinfer1::IExecutionContext&gt;(mEngine-&gt;<span class="built_in">createExecutionContext</span>());</span><br><span class="line"><span class="keyword">if</span> (!context)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-配置输入输出buffer"><a href="#2-配置输入输出buffer" class="headerlink" title="(2)配置输入输出buffer"></a>(2)配置输入输出buffer</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">samplesCommon::BufferManager <span class="title">buffers</span><span class="params">(mEngine)</span></span>;</span><br><span class="line"><span class="built_in">ASSERT</span>(mParams.inputTensorNames.<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">processInput</span>(buffers))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>BufferManager类用来处理host于device上buffer的分配与释放，简化buffer管理及buffer与engine之间的交互。</p><p>processInput(buffers)则用来读取输入，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SampleOnnxMNIST::processInput</span><span class="params">(<span class="keyword">const</span> samplesCommon::BufferManager&amp; buffers)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 获取尺寸</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> inputH = mInputDims.d[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> inputW = mInputDims.d[<span class="number">3</span>];</span><br><span class="line">    <span class="comment">// 随机获取输入图片</span></span><br><span class="line">    <span class="built_in">srand</span>(<span class="built_in"><span class="keyword">unsigned</span></span>(<span class="built_in">time</span>(<span class="literal">nullptr</span>)));</span><br><span class="line">    <span class="function">std::vector&lt;<span class="keyword">uint8_t</span>&gt; <span class="title">fileData</span><span class="params">(inputH * inputW)</span></span>;</span><br><span class="line">    mNumber = <span class="built_in">rand</span>() % <span class="number">10</span>;</span><br><span class="line">    <span class="built_in">readPGMFile</span>(<span class="built_in">locateFile</span>(std::<span class="built_in">to_string</span>(mNumber) + <span class="string">&quot;.pgm&quot;</span>, mParams.dataDirs), fileData.<span class="built_in">data</span>(), inputH, inputW);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印图片</span></span><br><span class="line">    sample::gLogInfo &lt;&lt; <span class="string">&quot;Input:&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; inputH * inputW; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        sample::gLogInfo &lt;&lt; (<span class="string">&quot; .:-=+*#%@&quot;</span>[fileData[i] / <span class="number">26</span>]) &lt;&lt; (((i + <span class="number">1</span>) % inputW) ? <span class="string">&quot;&quot;</span> : <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    sample::gLogInfo &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 读入输入数据，不采用float(fileData[i] / 255.0</span></span><br><span class="line">    <span class="comment">// 而采用1.0 - float(fileData[i] / 255.0的原因未知</span></span><br><span class="line">    <span class="keyword">float</span>* hostDataBuffer = <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>*&gt;(buffers.<span class="built_in">getHostBuffer</span>(mParams.inputTensorNames[<span class="number">0</span>]));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; inputH * inputW; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        hostDataBuffer[i] = <span class="number">1.0</span> - <span class="built_in"><span class="keyword">float</span></span>(fileData[i] / <span class="number">255.0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-执行推理"><a href="#3-执行推理" class="headerlink" title="(3)执行推理"></a>(3)执行推理</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将输入数据传到Device上</span></span><br><span class="line">buffers.<span class="built_in">copyInputToDevice</span>();</span><br><span class="line"><span class="comment">// 同步执行网络推理</span></span><br><span class="line"><span class="keyword">bool</span> status = context-&gt;<span class="built_in">executeV2</span>(buffers.<span class="built_in">getDeviceBindings</span>().<span class="built_in">data</span>());</span><br><span class="line"><span class="keyword">if</span> (!status)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将输出传回Host </span></span><br><span class="line">buffers.<span class="built_in">copyOutputToHost</span>();</span><br></pre></td></tr></table></figure><p>executeV2的输入为指向输入与输出buffer的一组指针。</p><h3 id="总体代码："><a href="#总体代码：" class="headerlink" title="总体代码："></a>总体代码：</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SampleOnnxMNIST::infer</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Create RAII buffer manager object</span></span><br><span class="line">    <span class="function">samplesCommon::BufferManager <span class="title">buffers</span><span class="params">(mEngine)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> context = SampleUniquePtr&lt;nvinfer1::IExecutionContext&gt;(mEngine-&gt;<span class="built_in">createExecutionContext</span>());</span><br><span class="line">    <span class="keyword">if</span> (!context)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read the input data into the managed buffers</span></span><br><span class="line">    <span class="built_in">ASSERT</span>(mParams.inputTensorNames.<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">processInput</span>(buffers))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Memcpy from host input buffers to device input buffers</span></span><br><span class="line">    buffers.<span class="built_in">copyInputToDevice</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> status = context-&gt;<span class="built_in">executeV2</span>(buffers.<span class="built_in">getDeviceBindings</span>().<span class="built_in">data</span>());</span><br><span class="line">    <span class="keyword">if</span> (!status)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Memcpy from device output buffers to host output buffers</span></span><br><span class="line">    buffers.<span class="built_in">copyOutputToHost</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verify results</span></span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">verifyOutput</span>(buffers))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a>执行结果：</h2><p><img src="https://s2.loli.net/2022/04/10/Ip4tKNlPREgq968.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;记录一下学习TensorRT的过程，基本上是提炼翻译了一下手册。&lt;/p&gt;
&lt;h1 id=&quot;一、TensorRT-简介&quot;&gt;&lt;a href=&quot;#一、TensorRT-简介&quot; class=&quot;headerlink&quot; title=&quot;一、TensorRT 简介&quot;&gt;&lt;/a&gt;一、Tenso</summary>
      
    
    
    
    <category term="TensorRT学习笔记" scheme="https://sognarere.github.io/categories/TensorRT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
